# -*- coding: utf-8 -*-
"""Earthquake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nooTxDmKfrQUTlkizizbUmHlem8oL_xG
"""

!apt-get install libgeos-3.5.0
!apt-get install libgeos-dev
!pip install https://github.com/matplotlib/basemap/archive/master.zip

!pip install pyproj==1.9.6

#IMPORTING ALL THE LIBRARIES

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import tensorflow as tf
from time import mktime as mktime
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Dropout,Reshape ,LSTM
from keras.optimizers import Adam,SGD
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf

#LOADING THE DATASET 

data = pd.read_csv("earthquake.csv")
print(data.columns)                                                            #OVERVIEW OF ALL THE COLUMNS 

data = data[['Date', 'Time', 'Latitude', 'Longitude', 'Depth', 'Magnitude']]
print(data.head())                                                              #COLUMNS TO BE CONSIDERED FOR THIS MODEL

print('Total lenth of this dataset is :',len(data.index))                       #TOTAL DATACOUNT 


#CONVERTING DATE AND TIME 
import datetime
import time
timestamp = []
for d, t in zip(data['Date'], data['Time']):
    try:
        ts = datetime.datetime.strptime(d+' '+t, '%m/%d/%Y %H:%M:%S')
        timestamp.append(time.mktime(ts.timetuple()))
    except ValueError:
        # print('ValueError')
        timestamp.append('ValueError')
timeStamp = pd.Series(timestamp)
data['Timestamp'] = timeStamp.values
final_data = data.drop(['Date', 'Time'], axis=1)
final_data = final_data[final_data.Timestamp != 'ValueError']
print(final_data.head())                                                        #MODIFIED DATASET 

X = final_data[['Timestamp', 'Latitude', 'Longitude']]                          #X AND y = INPUTDATA and TARGET VALUES
y = final_data[['Magnitude', 'Depth']]

#SPLITING THE DATASET INTO TRAIN AND TEST 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
#print(target)
#print(inputdata)
#scaler = MinMaxScaler()
#scaler.fit(X_train)
#scaler.fit(y_train)
#y_train = scaler.transform(y_train)
#y_test = scaler.transform(y_test)
from tensorflow.keras import regularizers

X_train = X_train.to_numpy(dtype=np.float)
X_test= X_test.to_numpy(dtype=np.float)
y_train = y_train.to_numpy(dtype=np.float)
y_test = y_test.to_numpy(dtype=np.float)

X_train = X_train.reshape(-1, 1, 3)
X_test  = X_test.reshape(-1, 1, 3)
y_train = y_train.reshape(-1, 1, 2)
y_test = y_test.reshape(-1, 1, 2)

from keras.regularizers import l2
model = Sequential()
model.add(LSTM(30,activation='relu',input_shape=(X_train.shape[1:]),return_sequences=True))
#model.add(Dropout(0.3))
model.add(SimpleRNN(20))
model.add(Dense(16, activation='relu',input_shape=(X_train.shape[1:]), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))
model.add(Dropout(0.2))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(2, activation='softmax'))

model.compile(optimizer='SGD', loss='squared_hinge', metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size=20, epochs=10, verbose=1, validation_data=(X_test, y_test))

[test_loss, test_acc] = model.evaluate(X_test, y_test)
print("Evaluation result on Test Data : Loss = {}, accuracy = {}".format(test_loss, test_acc))

#PREDICTION OF FUTURE ERTHQUAKE MAGNITUDE
#new_input =                                                    #enter a new inputdata in the format Timestamp , latitude , longitude
#new_input = np.asarray(new_input).astype('float32')
#new_input = new_input.reshape((len(new_input), 1, 3))
#prediction = model.predict(new_input, verbose=0)
#print(prediction)
#prediction = model.predict(new_input)

m = Basemap(projection='mill',llcrnrlat=-80,urcrnrlat=80, llcrnrlon=-180,urcrnrlon=180,lat_ts=20,resolution='c')

longitudes = data["Longitude"].tolist()
latitudes = data["Latitude"].tolist()
#m = Basemap(width=12000000,height=9000000,projection='lcc',
            #resolution=None,lat_1=80.,lat_2=55,lat_0=80,lon_0=-107.)
x,y = m(longitudes,latitudes)
fig = plt.figure(figsize=(12,10))
plt.title("All affected areas")

m.drawcoastlines()
m.fillcontinents(color='coral',lake_color='aqua')
m.drawmapboundary()
m.drawcountries()
m.plot(x, y, "o", markersize = 2, color = 'blue')
plt.show()

print(history.history.keys())
#  "Accuracy"
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()